# --- pipeline.py ---
# AI Mood Journal: Sentiment + Response Pipeline
# Phi√™n b·∫£n: v4.0 (simple emotion mapping + JSON logging)

import time
import torch
import torch.nn.functional as F

from core.hf_client import query_model
from core.config import (
    SENTIMENT_MODEL_NAME,
    ROUND_DECIMALS,
    SOFTMAX_TEMPERATURE,
    CHUNK_SIZE
)
from core.utils import (
    get_vn_timestamp,
    detect_sentiment_case,
    chunk_text,
)


# ======================== #
#  H√ÄM PH·ª§ TR·ª¢
# ======================== #
def print_pipeline_result(result):
    print("\n===== SENTIMENT PIPELINE RESULT =====")
    print(f"Input: {result['text']}")
    print(f"Predicted Label: {result['predicted_label']}")
    print(f"Distribution: {result['label_distribution']}")
    print(f"Emotion Detail: {result['emotion_detail']}")
    print(f"Case Type: {result['case_type']}")
    print(f"Advice: {result['advice_text']}")
    print(f"Timestamp: {result['timestamp']}")
    print("=====================================\n")


# ======================== #
#  PIPELINE CH√çNH
# ======================== #
def run_ai_pipeline(text: str, user_id: str = None):
    pipeline_start_time = get_vn_timestamp()
    start = time.time()
    print(f"‚è≥ Pipeline started at {pipeline_start_time}")

    # ------------------------------
    # B∆Ø·ªöC 1: CHUNKING
    # ------------------------------
    chunks = chunk_text(text, chunk_size=CHUNK_SIZE)
    print(f"üìò Chunked into {len(chunks)} parts")

    all_chunk_results = []
    all_probs = []

    # ------------------------------
    # B∆Ø·ªöC 2: SENTIMENT T·ª™NG CHUNK
    # ------------------------------
    for i, chunk in enumerate(chunks):
        print(f"\n--- Analyzing chunk {i+1}/{len(chunks)} ---")

        sentiment_result = query_model("sentiment", chunk)
        if "error" in sentiment_result:
            return {
                "status": "error",
                "error_message": f"Sentiment model failed on chunk {i+1}: {sentiment_result['error']}",
                "timestamp": pipeline_start_time,
            }
        # ==== FIX PROBS ROBUST ====

        def extract_probs(sr):
            """
            Tr·∫£ v·ªÅ [neg, neu, pos] trong m·ªçi tr∆∞·ªùng h·ª£p.
            """
            # CASE 1 ‚Äî model tr·∫£ probs
            if isinstance(sr.get("probs"), list) and len(sr["probs"]) == 3:
                p = sr["probs"]
                s = sum(p) or 1
                return [p[0]/s, p[1]/s, p[2]/s]

            # CASE 2 ‚Äî tr·∫£ label_distribution (%)
            dist = sr.get("label_distribution")
            if isinstance(dist, dict) and {"negative","neutral","positive"} <= set(dist):
                n = float(dist["negative"])
                u = float(dist["neutral"])
                p = float(dist["positive"])
                s = n + u + p or 1
                return [n/s, u/s, p/s]

            # CASE 3 ‚Äî tr·∫£ raw_logits
            logits = sr.get("raw_logits")
            if isinstance(logits, list) and len(logits) == 3:
                import math
                exp = [math.exp(x) for x in logits]
                s = sum(exp)
                return [exp[0]/s, exp[1]/s, exp[2]/s]

            # CASE 4 ‚Äî fallback
            return [0.0, 0.0, 0.0]


        # ---- d√πng extract_probs ƒë·ªÉ kh√¥ng l·ªói n·ªØa ----
        probs = extract_probs(sentiment_result)

        label_distribution = {
            "negative": round(probs[0]*100, 2),
            "neutral":  round(probs[1]*100, 2),
            "positive": round(probs[2]*100, 2)
        }

        label = sentiment_result.get("predicted_label", "unknown")


        # ------------------------------
        # SIMPLE EMOTION MAPPING
        # ------------------------------
        neg, neu, pos = probs

        if pos > 0.8:
            emotion_detail = "vui v·∫ª"
        elif pos > 0.5:
            emotion_detail = "t√≠ch c·ª±c nh·∫π"
        elif neu > 0.5:
            emotion_detail = "b√¨nh th∆∞·ªùng"
        elif neg > 0.8:
            emotion_detail = "r·∫•t ti√™u c·ª±c"
        elif neg > 0.5:
            emotion_detail = "bu·ªìn b√£"
        else:
            emotion_detail = "kh√¥ng r√µ"

        # label_distribution cho t·ª´ng chunk
        label_distribution = {
            "negative": round(probs[0] * 100, ROUND_DECIMALS),
            "neutral": round(probs[1] * 100, ROUND_DECIMALS),
            "positive": round(probs[2] * 100, ROUND_DECIMALS),
        }

        # Schema chu·∫©n cho detect_sentiment_case
        all_chunk_results.append({
            "text": chunk,
            "label": label,
            "probs": probs,
            "length": len(chunk),
            "emotion_detail": emotion_detail,
        })

        all_probs.append(probs)

    # ------------------------------
    # B∆Ø·ªöC 3: T·ªîNG H·ª¢P TO√ÄN VƒÇN B·∫¢N
    # ------------------------------
    avg_probs = torch.tensor(all_probs).mean(dim=0).tolist()
    label_distribution = {
        "negative": round(avg_probs[0] * 100, ROUND_DECIMALS),
        "neutral": round(avg_probs[1] * 100, ROUND_DECIMALS),
        "positive": round(avg_probs[2] * 100, ROUND_DECIMALS),
    }

    predicted_label = max(label_distribution, key=label_distribution.get)

    # ------------------------------
    # B∆Ø·ªöC 4: G·ªòP C·∫¢M X√öC
    # ------------------------------
    emotion_set = []
    for c in all_chunk_results:
        e = c["emotion_detail"]
        if e not in emotion_set:
            emotion_set.append(e)

    emotion_detail_summary = ", ".join(emotion_set) if emotion_set else "kh√¥ng r√µ"

    # ------------------------------
    # B∆Ø·ªöC 5: NH·∫¨N DI·ªÜN CASE
    # ------------------------------
    case_type = detect_sentiment_case(all_chunk_results)
    print(f"üß† Detected sentiment case: {case_type}")

    # ------------------------------
    # B∆Ø·ªöC 6: SINH PH·∫¢N H·ªíI
    # ------------------------------
    prompt = (
        f"Ng·ªØ c·∫£nh: {text}\n"
        f"C·∫£m x√∫c trung b√¨nh: {predicted_label} ({label_distribution})\n"
        f"M√¥ t·∫£: {emotion_detail_summary}\n"
        f"Lo·∫°i c·∫£m x√∫c: {case_type}\n"
        f"H√£y vi·∫øt m·ªôt ph·∫£n h·ªìi ng·∫Øn g·ªçn, ·∫•m √°p v√† t·ª± nhi√™n cho ng∆∞·ªùi d√πng."
    )

    response_result = query_model("response", prompt)

    if "error" in response_result:
        advice_text = "M√¨nh ƒëang h∆°i tr·ª•c tr·∫∑c m·ªôt ch√∫t, nh∆∞ng m√¨nh v·∫´n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n üåø."
        advice_source = "fallback"
    else:
        advice_text = response_result.get("text", "").strip()
        advice_source = response_result.get("source", "gemini_flash")

    # ------------------------------
    # B∆Ø·ªöC 7: K·∫æT QU·∫¢ CU·ªêI
    # ------------------------------
    final_result = {
        "status": "success",
        "text": text,
        "predicted_label": predicted_label,
        "label_distribution": label_distribution,
        "emotion_detail": emotion_detail_summary,
        "case_type": case_type,
        "advice_text": advice_text,
        "advice_source": advice_source,
        "timestamp": get_vn_timestamp(),
        "processing_time": round(time.time() - start, 2),
    }
    # ------------------------------
    # B∆Ø·ªöC 8: APPEND JSON L·ªäCH S·ª¨
    # ------------------------------
    import json, os
    save_path = "pipeline_history.json"

    # T·∫°o file n·∫øu ch∆∞a c√≥
    if not os.path.exists(save_path):
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)

    # Append k·∫øt qu·∫£ v√†o file
    with open(save_path, "r+", encoding="utf-8") as f:
        data = json.load(f)
        data.append(final_result)
        f.seek(0)
        json.dump(data, f, ensure_ascii=False, indent=2)

    return final_result
