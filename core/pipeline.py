# --- pipeline.py ---
# AI Mood Journal: Sentiment + Response Pipeline
# Phi√™n b·∫£n: v4.0 (simple emotion mapping + JSON logging)

import time
import torch
import torch.nn.functional as F
from core.history_engine import build_past_context
# üÜï To-Do Engine
from core.todo_engine import (
    extract_tasks_from_text,
    generate_gentle_question,
    create_todo_plan
)

from core.hf_client import query_model
from core.config import (
    SENTIMENT_MODEL_NAME,
    ROUND_DECIMALS,
    SOFTMAX_TEMPERATURE,
    CHUNK_SIZE
)
from core.utils import (
    get_vn_timestamp,
    detect_sentiment_case,
    chunk_text,
)


# ======================== #
#  H√ÄM PH·ª§ TR·ª¢
# ======================== #
def print_pipeline_result(result):
    print("\n===== SENTIMENT PIPELINE RESULT =====")
    print(f"Input: {result['text']}")
    print(f"Predicted Label: {result['predicted_label']}")
    print(f"Distribution: {result['label_distribution']}")
    print(f"Emotion Detail: {result['emotion_detail']}")
    print(f"Case Type: {result['case_type']}")
    print(f"Advice: {result['advice_text']}")
    print(f"Timestamp: {result['timestamp']}")
    print("=====================================\n")


# ======================== #
#  PIPELINE CH√çNH
# ======================== #
def run_ai_pipeline(text: str, user_id: str = None):
    pipeline_start_time = get_vn_timestamp()
    start = time.time()
    print(f"‚è≥ Pipeline started at {pipeline_start_time}")

    # ------------------------------
    # B∆Ø·ªöC 1: CHUNKING
    # ------------------------------
    chunks = chunk_text(text, chunk_size=CHUNK_SIZE)
    print(f"üìò Chunked into {len(chunks)} parts")

    all_chunk_results = []
    all_probs = []

    # ------------------------------
    # B∆Ø·ªöC 2: SENTIMENT T·ª™NG CHUNK
    # ------------------------------
    for i, chunk in enumerate(chunks):
        print(f"\n--- Analyzing chunk {i+1}/{len(chunks)} ---")

        sentiment_result = query_model("sentiment", chunk)
        if "error" in sentiment_result:
            return {
                "status": "error",
                "error_message": f"Sentiment model failed on chunk {i+1}: {sentiment_result['error']}",
                "timestamp": pipeline_start_time,
            }
        # ==== FIX PROBS ROBUST ====

        def extract_probs(sr):
            """
            Tr·∫£ v·ªÅ [neg, neu, pos] trong m·ªçi tr∆∞·ªùng h·ª£p.
            """
            # CASE 1 ‚Äî model tr·∫£ probs
            if isinstance(sr.get("probs"), list) and len(sr["probs"]) == 3:
                p = sr["probs"]
                s = sum(p) or 1
                return [p[0]/s, p[1]/s, p[2]/s]

            # CASE 2 ‚Äî tr·∫£ label_distribution (%)
            dist = sr.get("label_distribution")
            if isinstance(dist, dict) and {"negative","neutral","positive"} <= set(dist):
                n = float(dist["negative"])
                u = float(dist["neutral"])
                p = float(dist["positive"])
                s = n + u + p or 1
                return [n/s, u/s, p/s]

            # CASE 3 ‚Äî tr·∫£ raw_logits
            logits = sr.get("raw_logits")
            if isinstance(logits, list) and len(logits) == 3:
                import math
                exp = [math.exp(x) for x in logits]
                s = sum(exp)
                return [exp[0]/s, exp[1]/s, exp[2]/s]

            # CASE 4 ‚Äî fallback
            return [0.0, 0.0, 0.0]


        # ---- d√πng extract_probs ƒë·ªÉ kh√¥ng l·ªói n·ªØa ----
        probs = extract_probs(sentiment_result)

        label_distribution = {
            "negative": round(probs[0]*100, 2),
            "neutral":  round(probs[1]*100, 2),
            "positive": round(probs[2]*100, 2)
        }

        label = sentiment_result.get("predicted_label", "unknown")


        # ------------------------------
        # SIMPLE EMOTION MAPPING
        # ------------------------------
        neg, neu, pos = probs

        if pos > 0.8:
            emotion_detail = "vui v·∫ª"
        elif pos > 0.5:
            emotion_detail = "t√≠ch c·ª±c nh·∫π"
        elif neu > 0.5:
            emotion_detail = "b√¨nh th∆∞·ªùng"
        elif neg > 0.8:
            emotion_detail = "r·∫•t ti√™u c·ª±c" 
        elif neg > 0.5:
            emotion_detail = "bu·ªìn b√£"
        else:
            emotion_detail = "kh√¥ng r√µ"

        # label_distribution cho t·ª´ng chunk
        label_distribution = {
            "negative": round(probs[0] * 100, ROUND_DECIMALS),
            "neutral": round(probs[1] * 100, ROUND_DECIMALS),
            "positive": round(probs[2] * 100, ROUND_DECIMALS),
        }

        # Schema chu·∫©n cho detect_sentiment_case
        all_chunk_results.append({
            "text": chunk,
            "label": label,
            "probs": probs,
            "length": len(chunk),
            "emotion_detail": emotion_detail,
        })

        all_probs.append(probs)

    # ------------------------------
    # B∆Ø·ªöC 3: T·ªîNG H·ª¢P TO√ÄN VƒÇN B·∫¢N
    # ------------------------------
    avg_probs = torch.tensor(all_probs).mean(dim=0).tolist()
    label_distribution = {
        "negative": round(avg_probs[0] * 100, ROUND_DECIMALS),
        "neutral": round(avg_probs[1] * 100, ROUND_DECIMALS),
        "positive": round(avg_probs[2] * 100, ROUND_DECIMALS),
    }

    predicted_label = max(label_distribution, key=label_distribution.get)

    # ------------------------------
    # B∆Ø·ªöC 4: G·ªòP C·∫¢M X√öC
    # ------------------------------
    emotion_set = []
    for c in all_chunk_results:
        e = c["emotion_detail"]
        if e not in emotion_set:
            emotion_set.append(e)

    emotion_detail_summary = ", ".join(emotion_set) if emotion_set else "kh√¥ng r√µ"

    # ------------------------------
    # B∆Ø·ªöC 5: NH·∫¨N DI·ªÜN CASE
    # ------------------------------
    case_type = detect_sentiment_case(all_chunk_results)
    print(f"üß† Detected sentiment case: {case_type}")
    # ------------------------------
    # B∆Ø·ªöC 5.5: ƒê·ªåC L·∫†I HISTORY ‚Üí T·∫†O B·ªêI C·∫¢NH QU√Å KH·ª®
    # ------------------------------
    history_context = build_past_context(
        current_label=predicted_label,
        case_type=case_type,
        #user_id=user_id,
    )
    print("üìö History context:")
    print(history_context)
    # ============================================
    # üÜï B∆Ø·ªöC 5.6: TODO ENGINE ‚Äì PH√ÅT HI·ªÜN NHI·ªÜM V·ª§
    # ============================================

    todo_candidates = extract_tasks_from_text(
        text,
        context_tags=[case_type] if case_type else []
    )

    print("üÜï [To-Do] Nhi·ªám v·ª• ph√°t hi·ªán:", todo_candidates)

    todo_question = None
    best_task = None

    # N·∫øu c√≥ nhi·ªám v·ª• ‚Üí sinh c√¢u h·ªèi nh·∫π nh√†ng
    if todo_candidates:
        best_task = max(todo_candidates, key=lambda x: x.confidence)
        todo_question = generate_gentle_question(best_task)
        print("\nüÜï [To-Do] C√¢u h·ªèi nh·∫π nh√†ng:")
        print(todo_question)

    # ------------------------------
   # ------------------------------
   # ------------------------------
    # B∆Ø·ªöC 6: SINH PH·∫¢N H·ªíI V√Ä PH√ÇN LO·∫†I TOPIC (G·ªòP 2 TRONG 1)
    # ------------------------------
    import json # C·∫ßn import ƒë·ªÉ ƒë·ªçc JSON
    print("ü§ñ Calling unified model for response and topic...")

    # PROMPT N√ÄY PH·∫¢I KH·ªöP V·ªöI ƒê·ªäNH D·∫†NG TRONG prompts.py
    prompt = (
        f"N·ªòI DUNG HI·ªÜN T·∫†I: Ng∆∞·ªùi d√πng v·ª´a chia s·∫ª: \"{text}\"\n"
        f"C·∫£m x√∫c ch√≠nh ƒë∆∞·ª£c nh·∫≠n di·ªán l√†: {predicted_label} (Ph√¢n b·ªë: {label_distribution})\n"
        f"C·∫£m x√∫c chi ti·∫øt (n·∫øu c√≥): {emotion_detail_summary}\n"
        f"(B·ªëi c·∫£nh/Lo·∫°i c·∫£m x√∫c hi·ªán t·∫°i: {case_type})\n"
        f"(B·ªêI C·∫¢NH QU√Å KH·ª®: {history_context})\n"
        f"NHI·ªÜM V·ª§: D·ª±a v√†o c·∫£ B·ªêI C·∫¢NH v√† N·ªòI DUNG HI·ªÜN T·∫†I, h√£y ph·∫£n h·ªìi h·ªç."
    )
    # üÜï N·∫øu c√≥ nhi·ªám v·ª• ‚Üí y√™u c·∫ßu LLM h·ªèi user theo ki·ªÉu nh·∫π nh√†ng
    if todo_question:
        prompt += (
            "\n\n=== G·ª¢I √ù NHI·ªÜM V·ª§ ===\n"
            f"{todo_question}\n"
            "N·∫øu ng∆∞·ªùi d√πng ƒë·ªìng √Ω, h√£y tr·∫£ l·ªùi: 'Ok, m√¨nh t·∫°o k·∫ø ho·∫°ch nh√©!'\n"
        )

    # --- Kh·ªüi t·∫°o gi√° tr·ªã m·∫∑c ƒë·ªãnh ---
    fallback_advice = "M√¨nh ƒëang h∆°i tr·ª•c tr·∫∑c m·ªôt ch√∫t, nh∆∞ng m√¨nh v·∫´n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n üåø."
    advice_text = fallback_advice
    topic_label = "kh√¥ng x√°c ƒë·ªãnh"
    advice_source = "fallback"
    
    # --- G·ªçi Model 1 L·∫¶N DUY NH·∫§T ---
    response_result = query_model("response", prompt)

    if "error" in response_result:
        print(f"‚ö†Ô∏è L·ªói model response: {response_result['error']}")
        # L·ªói, gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh
    else:
        raw_text = response_result.get("text", "").strip()
        advice_source = response_result.get("source", "gemini_flash")
        # N·∫øu advice_text ch·ª©a JSON (v√¨ model c√≥ th·ªÉ tr·∫£ to√†n b·ªô trong 1 chu·ªói)
        if raw_text.strip().startswith("{") and "response" in raw_text:
            try:
                inner_data = json.loads(raw_text)
                advice_text = inner_data.get("response", fallback_advice).strip()
                topic_label = inner_data.get("topic", "kh√¥ng x√°c ƒë·ªãnh").strip()
                print(f"‚úÖ ƒê√£ t√°ch topic b√™n trong advice_text: {topic_label}")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói parse advice_text n·ªôi b·ªô: {e}")

        try:
                    # --- L·∫¶N 1: Parse JSON th·∫≥ng ---
            json_start = raw_text.find('{')
            json_end = raw_text.rfind('}') + 1
            if json_start == -1 or json_end == 0:
                raise json.JSONDecodeError("Kh√¥ng t√¨m th·∫•y JSON object", raw_text, 0)

            json_str = raw_text[json_start:json_end]

            # --- Chu·∫©n h√≥a kh√≥a JSON ---
            json_str = json_str.replace(" response:", ' "response":')
            json_str = json_str.replace("response:", ' "response":')
            json_str = json_str.replace(" topic:", ' "topic":')
            json_str = json_str.replace("topic:", ' "topic":')

            data = json.loads(json_str)

           # --- Parse v√† t√°ch JSON l·ªìng nhau ---
            try:
                # N·∫øu advice_text l√† JSON string, parse th√™m 1 l·ªõp n·ªØa
                advice_raw = data.get("response", "")
                if advice_raw.strip().startswith("{"):
                    inner = json.loads(advice_raw)
                    advice_text = inner.get("response", fallback_advice).strip()
                    topic_label = inner.get("topic", "kh√¥ng x√°c ƒë·ªãnh").strip()
                    print(f"‚úÖ ƒê√£ l·∫•y topic t·ª´ l·ªõp JSON trong advice_text: {topic_label}")
                else:
                    advice_text = advice_raw or fallback_advice
                    topic_label = data.get("topic", "kh√¥ng x√°c ƒë·ªãnh").strip()

            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi parse advice_text l·ªìng nhau: {e}")
                advice_text = fallback_advice
                topic_label = "kh√¥ng x√°c ƒë·ªãnh"

            if not advice_text:
                advice_text = fallback_advice
            print(f"üëç Response parsed (L·∫ßn 1).")

        except (json.JSONDecodeError, Exception) as e:
            # --- L·∫¶N 2: Th·ª≠ parse l·∫°i b·∫±ng c√°ch l√†m s·∫°ch chu·ªói ---
            print(f"‚ö†Ô∏è L·ªói parse JSON l·∫ßn 1 ({e}). Th·ª≠ l·∫°i v·ªõi replace...")
            try:
                # L√†m s·∫°ch k√Ω t·ª± ƒë·∫∑c bi·ªát v√† escape k√Ω t·ª± xu·ªëng d√≤ng
                cleaned = raw_text.replace("\r", "").replace("\n", "\\n").strip()
                cleaned = cleaned.replace("```json", "").replace("```", "")

                json_start = cleaned.find('{')
                json_end = cleaned.rfind('}') + 1
                if json_start == -1 or json_end <= 0:
                    raise ValueError("Kh√¥ng t√¨m th·∫•y JSON object trong l·∫ßn 2")

                json_str = cleaned[json_start:json_end]

                # Escape k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ªÉ tr√°nh l·ªói JSON
                import re
                json_str = re.sub(r'(?<=\{|,)\s*(\w+):', r'"\1":', json_str)

                # Th·ª≠ parse l·∫°i
                data = json.loads(json_str)

                advice_text = data.get("response", fallback_advice).strip()
                topic_label = data.get("topic", "kh√¥ng x√°c ƒë·ªãnh").strip()
                advice_source = response_result.get("source", "gemini_flash")
                print(f"‚úÖ JSON parsed th√†nh c√¥ng (L·∫ßn 2). Topic: {topic_label}")

            except Exception as e2:
                print(f"‚ö†Ô∏è L·ªói parse JSON l·∫ßn 2 ({e2}). Fallback v·ªÅ raw text.")
                advice_text = raw_text.replace("```json", "").replace("```", "").strip()
                if not advice_text:
                    advice_text = fallback_advice
                topic_label = "kh√¥ng x√°c ƒë·ªãnh"
                print(f"üëç Fallback: Set advice to raw text. Topic: {topic_label}")



                # --- FIX B·ªî SUNG: T·ª± ƒë·ªông tr√≠ch topic t·ª´ advice_text n·∫øu c√≤n d·∫°ng chu·ªói JSON ---
        if isinstance(advice_text, str) and advice_text.strip().startswith("{") and "topic" in advice_text:
            try:
                inner = json.loads(advice_text)
                advice_text = inner.get("response", advice_text).strip()
                topic_label = inner.get("topic", topic_label).strip()
                print(f"‚úÖ ƒê√£ t√°ch topic t·ª´ advice_text (fix cu·ªëi): {topic_label}")
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ parse topic trong advice_text (fix cu·ªëi): {e}")

        # ------------------------------
    # ------------------------------
    # ============================================
    # üÜï B∆Ø·ªöC 7: USER ƒê·ªíNG √ù ‚Üí T·∫†O TO-DO PLAN
    # ============================================
        todo_plan = None

        if todo_question:
            user_reply = advice_text.lower().strip()

            need_plan = any(k in user_reply for k in [
                "ok", "oke", "ƒë·ªìng √Ω", "t·∫°o k·∫ø ho·∫°ch", "l√†m ƒëi", "yes"
            ])

            if need_plan and best_task:
                todo_plan = create_todo_plan(best_task, text)
                print("üÜï [To-Do] ƒê√£ t·∫°o k·∫ø ho·∫°ch:", todo_plan)
 # B∆Ø·ªöC 8: K·∫æT QU·∫¢ CU·ªêI

    final_result = {
        "todo_candidates": [
    {
        "action": t.action,
        "description": t.description,
        "confidence": t.confidence,
        "context_tags": t.context_tags,
    }
    for t in todo_candidates
],

        "todo_question": todo_question,

        "todo_plan": {
            "main_task": todo_plan.main_task,
            "subtasks": todo_plan.subtasks,
            "deadline": todo_plan.deadline,
            "timeline": todo_plan.timeline,
        } if todo_plan else None,

        "status": "success",
        "text": text,
        "predicted_label": predicted_label,
        "label_distribution": label_distribution,
        "emotion_detail": emotion_detail_summary,
        "topic": topic_label,
        "case_type": case_type,
        "advice_text": advice_text,
        "advice_source": advice_source,
        "timestamp": get_vn_timestamp(),
        "processing_time": round(time.time() - start, 2),
    }
    
   # =========================
# B∆Ø·ªöC 8: APPEND L·ªäCH S·ª¨ KH√îNG GHI ƒê√à
# =========================
    import os, json

    save_path = "pipeline_history.json"

    try:
        # --- FIX TOPIC TR∆Ø·ªöC ---
        advice_raw = final_result.get("advice_text", "")
        topic_label = final_result.get("topic", "kh√¥ng x√°c ƒë·ªãnh")

        if isinstance(advice_raw, str) and advice_raw.strip().startswith("{"):
            try:
                inner = json.loads(advice_raw)
                final_result["advice_text"] = inner.get("response", advice_raw)
                final_result["topic"] = inner.get("topic", topic_label)
                print(f"‚úÖ ƒê√£ l·∫•y topic n·ªôi b·ªô: {final_result['topic']}")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói parse advice_text n·ªôi b·ªô: {e}")

        # --- T·∫†O FILE N·∫æU CH∆ØA C√ì ---
        if not os.path.exists(save_path):
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump([], f, ensure_ascii=False, indent=2)

        # --- APPEND D·ªÆ LI·ªÜU M·ªöI ---
        with open(save_path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if not isinstance(data, list):
                    data = []
            except json.JSONDecodeError:
                data = []

        # Th√™m b·∫£n ghi m·ªõi
        data.append(final_result)

        # --- GHI L·∫†I (OVERWRITE TO√ÄN B·ªò LIST) ---
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print("‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ m·ªõi v√†o pipeline_history.json")

    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ghi d·ªØ li·ªáu l·ªãch s·ª≠: {e}")

    return final_result

