"""
core/utils.py
-----------------------------------------
Chá»©a cÃ¡c hÃ m tiá»‡n Ã­ch dÃ¹ng chung giá»¯a:
- Team AI (pipeline, test)
- Team Backend (server, API)
-----------------------------------------
Má»¥c tiÃªu: tÃ¡i sá»­ dá»¥ng, trÃ¡nh láº·p code, dá»… báº£o trÃ¬.
"""

from datetime import datetime, timezone, timedelta
import json
import os
import re
from typing import Any, Dict, List
import numpy as np
import re

def chunk_text(text, chunk_size=300):
    """
    Chia vÄƒn báº£n dÃ i thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunk) theo cÃ¢u, cá»¥m ngá»¯ nghÄ©a hoáº·c tá»« ná»‘i.
    - chunk_size: Ä‘á»™ dÃ i tá»‘i Ä‘a má»—i chunk (tÃ­nh theo kÃ½ tá»±)
    """
    # TÃ¡ch dá»±a theo dáº¥u cÃ¢u hoáº·c tá»« ná»‘i
    sentences = re.split(r'(?<=[.!?]) +|(?<=, )|(?<= nhÆ°ng )|(?<= mÃ  )|(?<= nÃªn )', text)

    chunks = []
    current_chunk = ""

    for sent in sentences:
        if len(current_chunk) + len(sent) <= chunk_size:
            current_chunk += " " + sent
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sent

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

# ============================================================
#  1. HÃ m: get_vn_timestamp()
# ------------------------------------------------------------
# Láº¥y thá»i gian hiá»‡n táº¡i theo mÃºi giá» Viá»‡t Nam (UTC+7)
# DÃ¹ng Ä‘á»ƒ gáº¯n timestamp vÃ o log, response, hoáº·c database.
# ============================================================
def get_vn_timestamp() -> str:
    """Tráº£ vá» timestamp giá» Viá»‡t Nam (ISO 8601 format)."""
    vn_time = datetime.now(timezone.utc) + timedelta(hours=7)
    return vn_time.strftime("%Y-%m-%d %H:%M:%S")

# ============================================================
# 2. HÃ m: append_jsonl()
# ------------------------------------------------------------
# Ghi thÃªm 1 dÃ²ng dá»¯ liá»‡u JSON vÃ o file .jsonl
# DÃ¹ng cho test log hoáº·c backend ghi log ngÆ°á»i dÃ¹ng.
# ============================================================
def append_jsonl(path: str, data: Dict[str, Any]) -> None:
    """
    Ghi dá»¯ liá»‡u dáº¡ng JSON vÃ o file .jsonl
    Má»—i dÃ²ng = 1 JSON object.
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)
        f.write("\n")

# ============================================================
#  3. HÃ m: read_jsonl()
# ------------------------------------------------------------
# Äá»c toÃ n bá»™ dá»¯ liá»‡u tá»« file .jsonl -> list[dict]
# DÃ¹ng trong analyze_logs.py hoáº·c dashboard backend.
# ============================================================
def read_jsonl(path: str) -> List[Dict[str, Any]]:
    """Äá»c toÃ n bá»™ file .jsonl, tráº£ vá» danh sÃ¡ch cÃ¡c dict."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file log: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line.strip()) for line in f if line.strip()]

# ============================================================
#  4. HÃ m: clean_text()
# ------------------------------------------------------------
# LÃ m sáº¡ch text ngÆ°á»i dÃ¹ng nháº­p (xÃ³a kÃ½ tá»± lá»—i, khoáº£ng tráº¯ng thá»«a)
# DÃ¹ng trÆ°á»›c khi gá»­i text cho GPT Ä‘á»ƒ trÃ¡nh lá»—i encoding.
# ============================================================
def clean_text(text: str) -> str:
    """
    Chuáº©n hÃ³a text:
    - Loáº¡i bá» kÃ½ tá»± khÃ´ng in Ä‘Æ°á»£c
    - Giáº£m khoáº£ng tráº¯ng
    - XÃ³a kÃ½ tá»± Ä‘iá»u khiá»ƒn (áº©n)
    """
    text = re.sub(r"[\x00-\x1f\x7f-\x9f]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ============================================================
#  5. HÃ m: normalize_sentiment()
# ------------------------------------------------------------
# Chuáº©n hÃ³a nhÃ£n cáº£m xÃºc model tráº£ vá» cho thá»‘ng nháº¥t
# (vÃ­ dá»¥: "POS" â†’ "positive", "NEG" â†’ "negative", ...)
# ============================================================
def normalize_sentiment(label: str) -> str:
    """
    Chuyá»ƒn nhÃ£n sentiment vá» dáº¡ng thá»‘ng nháº¥t (chuáº©n hÃ³a)
    """
    mapping = {
        "POS": "positive",
        "NEG": "negative",
        "NEU": "neutral",
        "positive": "positive",
        "negative": "negative",
        "neutral": "neutral"
    }
    return mapping.get(label.strip().upper(), "neutral")

# ============================================================
#  6. HÃ m: aggregate_consistent()
# ------------------------------------------------------------
# Khi cÃ¡c chunk cÃ¹ng hÆ°á»›ng cáº£m xÃºc 
#  TÃ­nh weighted mean theo Ä‘á»™ dÃ i chunk
#VÃ­ dá»¥ CÃ³ 2 chunk cÃ¹ng positive [0.8, 0.9] â†’ posâ‰ˆ0.85
# ============================================================
def aggregate_consistent(chunks: List[Dict[str, Any]]) -> np.ndarray:
    """
    âœ… Äá»‹nh nghÄ©a:
        Khi cÃ¡c chunk Ä‘á»u cÃ³ cáº£m xÃºc cÃ¹ng hÆ°á»›ng (Ä‘á»u pos hoáº·c neg).
    âš™ï¸ Dáº¥u hiá»‡u:
        - CÃ¡c label trÃ¹ng nhau hoáº·c tÆ°Æ¡ng Ä‘á»“ng >80%.
    ğŸ§® Chiáº¿n lÆ°á»£c:
        Weighted mean theo Ä‘á»™ dÃ i chunk.
    ğŸ¯ Output:
        Vector xÃ¡c suáº¥t trung bÃ¬nh (3 lá»›p).
    ğŸ“˜ VÃ­ dá»¥:
        [pos=0.8, pos=0.9] â†’ posâ‰ˆ0.85
    """
    total_weight, weighted_probs = 0, np.zeros(3)
    for c in chunks:
        w = c["length"]
        weighted_probs += np.array(c["probs"]) * w
        total_weight += w
    return weighted_probs / total_weight


# ============================================================
#  7. HÃ m: aggregate_mild_shift()
# ------------------------------------------------------------
# Khi cÃ¡c chunk lá»‡ch nháº¹ (positive â†” neutral).
#Khi cÃ¢u vÄƒn xÃ©t trÃªn 1 váº¥n Ä‘á»/Ä‘á»‘i tÆ°á»£ng/khÃ­a cáº¡nh nhÆ°ng cÃ³ sá»± thay Ä‘á»•i cáº£m xÃºc nháº¹ tá»« pos/neg sang neutral hoáº·c ngÆ°á»£c láº¡i.
#KhÃ´ng cÃ³ sá»± Ä‘áº£o cá»±c tá»« positive sang negative hoáº·c ngÆ°á»£c láº¡i.  
#VÃ­ dá»¥ A cÃ³ 2 chunk: 
#chunk 1: positive [0.7, 0.2, 0.1] cÃ³ Ä‘á»™ dÃ i 20
#chunk 2: neutral [0.3, 0.1, 0.6] ( pos/neu/neg) cÃ³ Ä‘á»™ dÃ i 30
# â†’ cáº£m xÃºc nghiÃªng nháº¹ vá» positive do chunk 1 cÃ³ Ä‘á»™ dÃ i ngáº¯n hÆ¡n
#TÃ­nh toÃ¡n probs cá»§a VD A:
#TÃ­nh Ä‘á»™ rÃµ nÃ©t cá»§a má»—i chunk: cÃ´ng thá»©c I = max_prob - mean_prob
#I1 = 0.7 - (0.7+0.2+0.1)/3 = 0.4667
#I2 = 0.6 - (0.3+0.1+0.6)/3 = 0.4
#Trá»ng sá»‘ w = Ä‘á»™ dÃ i * (1 + I)
#w1 = 20 * (1 + 0.4667) = 29.334
#w2 = 30 * (1 + 0.4) = 42   
#Tá»•ng trá»ng sá»‘ w = w1 + w2 = 71.334
#TÃ­nh weighted mean probs: probs_label = Tá»•ng (probs_chuck_i * w_i) / Tá»•ng trá»ng sá»‘ w ( i cháº¡y tá»« 1 tá»›i n chunk)
#pos = (0.7 * 29.334 + 0.3 * 42) / 71.334 = 0.487
#neu = (0.2 * 29.334 + 0.1 * 42) / 71.334 = 0.147
#neg = (0.1 * 29.334 + 0.6 * 42) / 71.334 = 0.366
# ============================================================
def aggregate_mild_shift(chunks: List[Dict[str, Any]]) -> np.ndarray:
    """
    âœ… Äá»‹nh nghÄ©a:
        Khi cáº£m xÃºc lá»‡ch nháº¹, khÃ´ng Ä‘áº£o cá»±c.
    âš™ï¸ Dáº¥u hiá»‡u:
        - CÃ¡c nhÃ£n khÃ¡c nhau nhÆ°ng khÃ´ng Ä‘á»‘i cá»±c.
        - ChÃªnh xÃ¡c suáº¥t top-2 < 0.4
    ğŸ§® Chiáº¿n lÆ°á»£c:
        Weighted mean + intensity (1 + Ä‘á»™ tá»± tin)
    ğŸ¯ Output:
        Cáº£m xÃºc nghiÃªng nháº¹ vá» phÃ­a máº¡nh hÆ¡n.
    """
    total_weight, weighted_probs = 0, np.zeros(3)
    for c in chunks:
        probs = np.array(c["probs"])
        intensity = np.max(probs) - np.mean(probs)
        w = c["length"] * (1 + intensity)
        weighted_probs += probs * w
        total_weight += w
    return weighted_probs / total_weight


# ============================================================
#  8. HÃ m: aggregate_polarity_shift()
# ------------------------------------------------------------
# Khi cáº£m xÃºc Ä‘áº£o cá»±c rÃµ rÃ ng â†’ láº¥y chunk sau cÃ¹ng.
#VÃ­ dá»¥ CÃ³ 2 chunk:
#chunk 1: positive [0.9, 0.05, 0.05]
#chunk 2: negative [0.1, 0.1, 0.8]
# â†’ cáº£m xÃºc chÃ­nh lÃ  negative (chunk 2) 
#Suy ra probs cá»§a VD:
#probs = [0.1, 0.1, 0.8]
# ============================================================
def aggregate_polarity_shift(chunks: List[Dict[str, Any]]) -> np.ndarray:
    """
    âœ… Äá»‹nh nghÄ©a:
        Khi cáº£m xÃºc Ä‘áº£o cá»±c rÃµ (vui rá»“i buá»“n).
    âš™ï¸ Dáº¥u hiá»‡u:
        - CÃ³ liÃªn tá»« Ä‘áº£o chiá»u (â€œnhÆ°ngâ€, â€œtuy nhiÃªnâ€).
        - Chunk cuá»‘i mang hÆ°á»›ng trÃ¡i ngÆ°á»£c.
    ğŸ§® Chiáº¿n lÆ°á»£c:
        Láº¥y xÃ¡c suáº¥t chunk sau cÃ¹ng.
    ğŸ¯ Output:
        Cáº£m xÃºc chÃ­nh (label chunk cuá»‘i).
    """
    return chunks[-1]["probs"]


# ============================================================
#  9. HÃ m: aggregate_uncertain()
# ------------------------------------------------------------
# Khi model khÃ´ng cháº¯c cháº¯n â†’ neutral + flag.
#VÃ­ dá»¥ CÃ³ 2 chunk:
#chunk 1: neutral [0.4, 0.3, 0.3]
#chunk 2: neutral [0.35, 0.4, 0.25]
# â†’ cáº£m xÃºc chÃ­nh lÃ  neutral (low confidence)   
#TÃ­nh probs cá»§a VD:
#probs = [0.33, 0.34, 0.33]
# ============================================================
def aggregate_uncertain(chunks: List[Dict[str, Any]]):
    """
    âœ… Äá»‹nh nghÄ©a:
        Khi xÃ¡c suáº¥t cÃ¡c nhÃ£n gáº§n nhau.
    âš™ï¸ Dáº¥u hiá»‡u:
        - max_prob - second_max_prob < 0.1
    ğŸ§® Chiáº¿n lÆ°á»£c:
        Tráº£ vá» neutral (low confidence)
    ğŸ¯ Output:
        ([neutral=1.0], "uncertain")
    """
    return np.array([0.33, 0.34, 0.33]), "uncertain"


# ============================================================
#  10. HÃ m: aggregate_multi_sentiment()
# ------------------------------------------------------------
# Khi cÃ³ >=2 cáº£m xÃºc máº¡nh Ä‘á»™c láº­p â†’ dÃ¹ng summarization.
#DÃ¹ng khi cÃ¢u cÃ³ chá»©a 2 váº¥n Ä‘á», 2 khÃ­a cáº¡nh, 2 Ä‘á»‘i tÆ°á»£ng khÃ¡c nhau vá»›i cáº£m xÃºc riÃªng biá»‡t, khÃ¡c nhau.
#VÃ­ dá»¥ CÃ³ 2 chunk: 
#chunk 1: positive [0.8, 0.1, 0.1] cÃ³ Ä‘á»™ dÃ i 25
#chunk 2: negative [0.1, 0.2, 0.7] cÃ³ Ä‘á»™ dÃ i 35
# â†’ cáº£m xÃºc chÃ­nh lÃ  mixed_sentiment do cÃ³ 2 cáº£m xÃºc máº¡nh song song 
#TÃ­nh probs cá»§a VD: ( CÃ¡ch tÃ­nh tÆ°Æ¡ng tá»± aggregate_consistent)
#probs = [0.45, 0.15, 0.4]
# ============================================================
def aggregate_multi_sentiment(chunks: List[Dict[str, Any]]):
    """
    âœ… Äá»‹nh nghÄ©a:
        CÃ³ >=2 cáº£m xÃºc máº¡nh Ä‘á»™c láº­p, khÃ´ng phá»§ Ä‘á»‹nh nhau.
    âš™ï¸ Dáº¥u hiá»‡u:
        - â‰¥2 chunk cÃ³ xÃ¡c suáº¥t > 0.6 á»Ÿ 2 label khÃ¡c nhau.
    ğŸ§® Chiáº¿n lÆ°á»£c:
        Trung bÃ¬nh cÃ³ trá»ng sá»‘ + gáº¯n flag â€œmixed_sentimentâ€.
    ğŸ¯ Output:
        (avg_probs, "mixed_sentiment")
    """
    total_weight, weighted_probs = 0, np.zeros(3)
    for c in chunks:
        w = c["length"]
        weighted_probs += np.array(c["probs"]) * w
        total_weight += w
    avg_probs = weighted_probs / total_weight
    return avg_probs, "mixed_sentiment"
# ============================================================
# 11. HÃ m: detect_sentiment_case()
# ------------------------------------------------------------
# Nháº­n vÃ o danh sÃ¡ch cÃ¡c chunk (má»—i chunk cÃ³ probs, label, length, text)
# Tráº£ vá» loáº¡i trÆ°á»ng há»£p cáº£m xÃºc:
#   - consistent
#   - mild_shift
#   - polarity_shift
#   - uncertain
#   - multi_sentiment
# ============================================================

def detect_sentiment_case(chunks: list) -> str:
    """
    âœ… Má»¥c Ä‘Ã­ch:
        Tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh loáº¡i cáº£m xÃºc cá»§a vÄƒn báº£n dá»±a trÃªn káº¿t quáº£ cÃ¡c chunk.

    âœ… Input (vÃ­ dá»¥):
        chunks = [
            {"text": "HÃ´m nay tÃ´i ráº¥t vui", "probs": [0.82, 0.12, 0.06], "label": "positive", "length": 12},
            {"text": "nhÆ°ng tÃ´i khÃ´ng vá» nhÃ  nÃªn hÆ¡i nhá»› máº¹", "probs": [0.08, 0.20, 0.72], "label": "negative", "length": 29}
        ]

    âœ… Output:
        "consistent", "mild_shift", "polarity_shift", "uncertain", hoáº·c "multi_sentiment"

    ------------------------------------------------------------
    âš™ï¸ Logic phÃ¡t hiá»‡n:

    1ï¸âƒ£ UNCERTAIN
        - Náº¿u max_prob - second_max_prob < 0.1 vá»›i háº§u háº¿t chunk
        â†’ model khÃ´ng cháº¯c cháº¯n â†’ "uncertain"

    2ï¸âƒ£ CONSISTENT
        - Náº¿u táº¥t cáº£ label giá»‘ng nhau (hoáº·c sai khÃ¡c <15%)
        â†’ cÃ¹ng hÆ°á»›ng cáº£m xÃºc â†’ "consistent"

    3ï¸âƒ£ POLARITY SHIFT
        - Náº¿u cÃ³ tá»« ná»‘i chuyá»ƒn hÆ°á»›ng (nhÆ°ng, tuy nhiÃªn, song, máº·c dÃ¹, trÃ¡i láº¡i)
          vÃ  label chunk cuá»‘i khÃ¡c háº³n label Ä‘áº§u (pos â†” neg)
        â†’ "polarity_shift"

    4ï¸âƒ£ MULTI SENTIMENT
        - Náº¿u cÃ³ â‰¥2 chunk cÃ³ nhÃ£n khÃ¡c nhau vá»›i xÃ¡c suáº¥t > 0.6
          mÃ  khÃ´ng cÃ³ tá»« ná»‘i phá»§ Ä‘á»‹nh â†’ song song nhiá»u cáº£m xÃºc
        â†’ "multi_sentiment"

    5ï¸âƒ£ MILD SHIFT
        - Náº¿u khÃ¡c nhau nháº¹ (positive â†” neutral hoáº·c neutral â†” negative)
        â†’ "mild_shift"
    ------------------------------------------------------------
    """

    # ---- Tiá»n xá»­ lÃ½
    labels = [c["label"] for c in chunks]
    probs = [c["probs"] for c in chunks]
    texts = " ".join([c["text"] for c in chunks]).lower()

    # ---- 1. TrÆ°á»ng há»£p uncertain
    def is_uncertain_chunk(p):
        sorted_p = sorted(p, reverse=True)
        return (sorted_p[0] - sorted_p[1]) < 0.1
    if all(is_uncertain_chunk(p) for p in probs):
        return "uncertain"

    # ---- 2. TrÆ°á»ng há»£p consistent
    if len(set(labels)) == 1:
        return "consistent"

    # ---- 3. TrÆ°á»ng há»£p polarity shift
    connectors = ["nhÆ°ng", "tuy nhiÃªn", "song", "trÃ¡i láº¡i", "máº·c dÃ¹"]
    if any(conn in texts for conn in connectors):
        if labels[0] != labels[-1] and (
            ("positive" in labels and "negative" in labels)
        ):
            return "polarity_shift"

    # ---- 4. TrÆ°á»ng há»£p multi_sentiment
    strong_chunks = [c for c in chunks if max(c["probs"]) > 0.6]
    strong_labels = list({c["label"] for c in strong_chunks})
    if len(strong_labels) >= 2:
        return "multi_sentiment"

    # ---- 5. TrÆ°á»ng há»£p mild_shift
    return "mild_shift"